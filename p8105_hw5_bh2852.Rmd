---
title: "p8105_hw5_bh2852"
author: "Beicheng Huang"
date: "2023-11-16"
output: github_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

### Problem 0

```{r load_libraries}
library(tidyverse)
```


## Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides. 




##Quesetion 2


```{r create dataframe and function}

data_col = tibble(filename = list.files("./data/"))

read_all = function(csv, path = "./data/") {
  paste(path, csv, sep = "/") |> 
  read.csv()
}


long_study =  
  data_col |> 
  mutate(nested_data = map(data_col$filename, read_all)) |> 
  #list of datasets to apply read_all function to
  unnest(nested_data) |> 
  #unnests data into all 8 columns
  mutate(arm_subject = map_chr(.x = filename, ~ unlist(str_split(.x, "\\.") ) [[1]] )) |> 
  #create new column to rename subjects by splitting with '.' and using only the first part
  mutate(arm = map_chr(.x = arm_subject, ~ unlist(str_split(.x, "_") ) [[1]] )) |> 
  mutate(subject_id = map_chr(.x = arm_subject, ~ unlist(str_split(.x, "_") ) [[2]] )) |> 
  #separate into arm and subject id columns
  select(-filename, -arm_subject)
  #only keep necessary columns of tidy dataset
```


```{r tidying and plotting}
#pivot longer data in order to create graph over time
pivot_longer = 
  long_study |> 
  pivot_longer(week_1:week_8,
               names_to = "week",
               names_prefix = "week_", #removes the week prefix in the week values
               values_to = "value") |> 
  mutate(week = as.integer(week))

spaghetti_plot = 
  pivot_longer |> 
  ggplot(aes(week, value, color = subject_id)) + 
  geom_line() + 
  facet_wrap(~arm) + 
  labs(x = 'Week', y = 'Value', title = 'Spaghetti Plot', color = "Subject ID")

spaghetti_plot
```

##Q3


```{r Set Parameters}
set.seed(1)

n <- 30
sigma <- 5
alpha <- 0.05
mius <- c(0, 1, 2, 3, 4, 5, 6)

```


```{r Define functions for miu and sigma}
sim_mean_sd = function(n, miu, sigma) {
  simulation = tibble(
    x = rnorm(n, mean = miu, sd = sigma),
  )
  
  simulation |> 
    summarize(
      miu_hat = mean(x),
      sigma_hat = sd(x)
    )
}
```


```{r Define a function for the t-test}
perform_t_test <- function(n, miu, sigma) {
  sample <- rnorm(n, mean = miu, sd = sigma)
  test_result <- t.test(sample, mu = 0)
  broom::tidy(test_result)
}
```


```{r Initialize data frame to store results}
results <- tibble(miu = numeric(), miu_hat = numeric(), p_value = numeric(), reject = logical())
```


```{r Simulation loop}
for (miu in mius) {
  for (i in 1:5000) {
    sim_results <- sim_mean_sd(n, miu, sigma)
    t_test_results <- perform_t_test(n, miu, sigma)
    results <- results |> 
      add_row(miu = miu, 
              miu_hat = sim_results$miu_hat, 
              p_value = t_test_results$p.value, 
              reject = t_test_results$p.value < alpha)
  }
}
```


```{r Estimated data}
power_results <- results |> 
  group_by(miu) |> 
  summarise(power = mean(reject), 
            avg_miu_hat = mean(miu_hat), 
            avg_miu_hat_rejected = mean(miu_hat[reject]))
```
  

```{r Plot1}
power_results |> 
  ggplot(aes(x = miu, y = power)) +
  geom_point() +
  geom_line() +
  labs(title = "Power vs. True Mean", x = "True Mean (miu)", y = "Power")
```
This graph shows the relation between the proportion of times that null was rejected on the y axis and true mean of μ on the x axis. As the true mean increases, the proportion of times that null will be rejected increases, thus the power of the test increases and approaches 1. This means that the larger the true means, the easier to reject the null hypothesis, thus having greater power.

```{r}

estimates_data <-
  sim_results |> 
  group_by(miu) |> 
  summarize(avg_miu_hat = mean(miu_hat))

estimates_rejection_df =
  sim_results |> 
  mutate(decision = ifelse(p.value < 0.05, "reject", "fail to reject")) |> 
  filter(decision == "reject") |> 
  group_by(miu, decision) |> 
  summarize(avg_miu_hat_rejected = mean(miu_hat))
```

```{r}
plot2 = 
  ggplot() + 
  geom_point(data = estimates_data, aes(x = mu_vals, y = sample_mean_estimates, color = "True Estimate")) + 
  geom_line(data = estimates_data, aes(x = mu_vals, y = sample_mean_estimates, color = "True Estimate")) +
  geom_line(data = estimates_rejection_df, 
            aes(x = mu_vals, y = sample_mean_estimates, color = "Rejected Estimates")) + 
  geom_point(data = estimates_rejection_df, 
             aes(x = mu_vals, y = sample_mean_estimates, color = "Rejected Estimates")) + 
  labs(x = 'True Mean (μ)', y = 'Sample Mean (μ) Estimates', 
       title = 'Rejected Average μ vs. True μ', color = "Color Coding:") + 
  scale_color_manual(values = c("Rejected Estimates" = "tomato", "True Estimate" = "darkgreen")) + 
  
plot2

```



```{r Plot2 average estimate miu_hat vs miu}
power_results |> 
  ggplot(aes(x = miu)) +
  geom_point(aes(y = avg_miu_hat, color = "Average Estimate of μ"), shape = 1) +  
  geom_line(aes(y = avg_miu_hat, color = "Average Estimate of μ")) +
  geom_point(aes(y = avg_miu_hat_rejected, color = "Conditional Average Estimate of μ"), shape = 2) +
  geom_line(aes(y = avg_miu_hat_rejected, color = "Conditional Average Estimate of μ"), linetype = "dashed") +
  labs(
    title = "Average Estimated Mean vs. True Mean",
    x = "True Mean (miu)",
    y = "Average Estimated Mean",
    color = "miu_hat"
  ) +
  scale_color_manual(
    values = c("Average Estimate of μ" = "blue", "Conditional Average Estimate of μ" = "red")
  ) 
```
